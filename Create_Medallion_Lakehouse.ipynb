{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3a62d7",
   "metadata": {},
   "source": [
    "# Create a medallion architecture in a Microsoft Fabric lakehouse\n",
    "https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/03b-medallion-lakehouse.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064a365",
   "metadata": {},
   "source": [
    "## Create a lakehouse \n",
    "1. upload data to bronze layer (created new subfolder named \"bronze\")\n",
    "2. create notebook from bronze folder and structure table and add metadata to table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "    \n",
    "# Create the schema for the table\n",
    "orderSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    StructField(\"OrderDate\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"Item\", StringType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"UnitPrice\", FloatType()),\n",
    "    StructField(\"Tax\", FloatType())\n",
    "    ])\n",
    "    \n",
    "# Import all files from bronze folder of lakehouse\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"false\").schema(orderSchema).load(\"Files/bronze/*.csv\")\n",
    "    \n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "display(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddae59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit, col, current_timestamp, input_file_name\n",
    "    \n",
    "# Add columns IsFlagged, CreatedTS and ModifiedTS\n",
    "df = df.withColumn(\"FileName\", input_file_name()) \\\n",
    "    .withColumn(\"IsFlagged\", when(col(\"OrderDate\") < '2019-08-01',True).otherwise(False)) \\\n",
    "    .withColumn(\"CreatedTS\", current_timestamp()).withColumn(\"ModifiedTS\", current_timestamp())\n",
    "    \n",
    "# Update CustomerName to \"Unknown\" if CustomerName null or empty\n",
    "df = df.withColumn(\"CustomerName\", when((col(\"CustomerName\").isNull() | (col(\"CustomerName\")==\"\")),lit(\"Unknown\")).otherwise(col(\"CustomerName\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946a8ce",
   "metadata": {},
   "source": [
    "## Update schema for silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the sales_silver table\n",
    "    \n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "    \n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"sales.sales_silver\") \\\n",
    "    .addColumn(\"SalesOrderNumber\", StringType()) \\\n",
    "    .addColumn(\"SalesOrderLineNumber\", IntegerType()) \\\n",
    "    .addColumn(\"OrderDate\", DateType()) \\\n",
    "    .addColumn(\"CustomerName\", StringType()) \\\n",
    "    .addColumn(\"Email\", StringType()) \\\n",
    "    .addColumn(\"Item\", StringType()) \\\n",
    "    .addColumn(\"Quantity\", IntegerType()) \\\n",
    "    .addColumn(\"UnitPrice\", FloatType()) \\\n",
    "    .addColumn(\"Tax\", FloatType()) \\\n",
    "    .addColumn(\"FileName\", StringType()) \\\n",
    "    .addColumn(\"IsFlagged\", BooleanType()) \\\n",
    "    .addColumn(\"CreatedTS\", DateType()) \\\n",
    "    .addColumn(\"ModifiedTS\", DateType()) \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c3f40",
   "metadata": {},
   "source": [
    "### upsert operation on a Delta table, \n",
    "updating existing records based on specific conditions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa1b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update existing records and insert new ones based on a condition defined by the columns SalesOrderNumber, OrderDate, CustomerName, and Item.\n",
    "\n",
    "from delta.tables import *\n",
    "    \n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/sales_silver')\n",
    "    \n",
    "dfUpdates = df\n",
    "    \n",
    "deltaTable.alias('silver') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'silver.SalesOrderNumber = updates.SalesOrderNumber and silver.OrderDate = updates.OrderDate and silver.CustomerName = updates.CustomerName and silver.Item = updates.Item'\n",
    "  ) \\\n",
    "   .whenMatchedUpdate(set =\n",
    "    {\n",
    "          \n",
    "    }\n",
    "  ) \\\n",
    " .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"SalesOrderNumber\": \"updates.SalesOrderNumber\",\n",
    "      \"SalesOrderLineNumber\": \"updates.SalesOrderLineNumber\",\n",
    "      \"OrderDate\": \"updates.OrderDate\",\n",
    "      \"CustomerName\": \"updates.CustomerName\",\n",
    "      \"Email\": \"updates.Email\",\n",
    "      \"Item\": \"updates.Item\",\n",
    "      \"Quantity\": \"updates.Quantity\",\n",
    "      \"UnitPrice\": \"updates.UnitPrice\",\n",
    "      \"Tax\": \"updates.Tax\",\n",
    "      \"FileName\": \"updates.FileName\",\n",
    "      \"IsFlagged\": \"updates.IsFlagged\",\n",
    "      \"CreatedTS\": \"updates.CreatedTS\",\n",
    "      \"ModifiedTS\": \"updates.ModifiedTS\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a081c",
   "metadata": {},
   "source": [
    "### Explore data in the silver layer using the SQL endpoint\n",
    "Select the Sales SQL analytics endpoint to open your lakehouse in the SQL analytics endpoint view.\n",
    "\n",
    "explore silver table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3434e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT YEAR(OrderDate) AS Year\n",
    "    , CAST (SUM(Quantity * (UnitPrice + Tax)) AS DECIMAL(12, 2)) AS TotalSales\n",
    "FROM sales_silver\n",
    "GROUP BY YEAR(OrderDate) \n",
    "ORDER BY YEAR(OrderDate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6c986",
   "metadata": {},
   "source": [
    "## Transform data for gold layer\n",
    "1. workspace home page and create a new notebook\n",
    "2. drop duplicates\n",
    "3. handle new data\n",
    "4. create gold table\n",
    "5. create new columns for gold\n",
    "6. create date dimension\n",
    "7. create customer dimension\n",
    "8. create product dimmension\n",
    "- a. create your gold product dimension dimProduct_gold\n",
    "- b. create the product_silver dataframe.\n",
    "- c. create IDs for your dimProduct_gold table\n",
    "- d. create automatic update of the dimenssion\n",
    "9. create fact table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dea8f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to the dataframe as a starting point to create the gold layer\n",
    "df = spark.read.table(\"laketest.sales_silver\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import*\n",
    "    \n",
    "# Define the schema for the dimdate_gold table\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"laketest.dimdate_gold\") \\\n",
    "    .addColumn(\"OrderDate\", DateType()) \\\n",
    "    .addColumn(\"Day\", IntegerType()) \\\n",
    "    .addColumn(\"Month\", IntegerType()) \\\n",
    "    .addColumn(\"Year\", IntegerType()) \\\n",
    "    .addColumn(\"mmmyyyy\", StringType()) \\\n",
    "    .addColumn(\"yyyymm\", StringType()) \\\n",
    "    .execute()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3d1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, dayofmonth, month, year, date_format\n",
    "    \n",
    "# Create dataframe for dimDate_gold\n",
    "    \n",
    "dfdimDate_gold = df.dropDuplicates([\"OrderDate\"]).select(col(\"OrderDate\"), \\\n",
    "        dayofmonth(\"OrderDate\").alias(\"Day\"), \\\n",
    "        month(\"OrderDate\").alias(\"Month\"), \\\n",
    "        year(\"OrderDate\").alias(\"Year\"), \\\n",
    "        date_format(col(\"OrderDate\"), \"MMM-yyyy\").alias(\"mmmyyyy\"), \\\n",
    "        date_format(col(\"OrderDate\"), \"yyyyMM\").alias(\"yyyymm\"), \\\n",
    "    ).orderBy(\"OrderDate\")\n",
    "\n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "\n",
    "display(dfdimDate_gold.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d790c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "    \n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/dimdate_gold')\n",
    "    \n",
    "dfUpdates = dfdimDate_gold\n",
    "    \n",
    "deltaTable.alias('gold') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'gold.OrderDate = updates.OrderDate'\n",
    "  ) \\\n",
    "   .whenMatchedUpdate(set =\n",
    "    {\n",
    "          \n",
    "    }\n",
    "  ) \\\n",
    " .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"OrderDate\": \"updates.OrderDate\",\n",
    "      \"Day\": \"updates.Day\",\n",
    "      \"Month\": \"updates.Month\",\n",
    "      \"Year\": \"updates.Year\",\n",
    "      \"mmmyyyy\": \"updates.mmmyyyy\",\n",
    "      \"yyyymm\": \"updates.yyyymm\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7415dbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "    \n",
    "# Create customer_gold dimension delta table\n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"laketest.dimcustomer_gold\") \\\n",
    "    .addColumn(\"CustomerName\", StringType()) \\\n",
    "    .addColumn(\"Email\",  StringType()) \\\n",
    "    .addColumn(\"First\", StringType()) \\\n",
    "    .addColumn(\"Last\", StringType()) \\\n",
    "    .addColumn(\"CustomerID\", LongType()) \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf08ab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col, when, coalesce, max, lit\n",
    "    \n",
    "dfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\n",
    "    \n",
    "MAXCustomerID = dfdimCustomer_temp.select(coalesce(max(col(\"CustomerID\")),lit(0)).alias(\"MAXCustomerID\")).first()[0]\n",
    "    \n",
    "dfdimCustomer_gold = dfdimCustomer_silver.join(dfdimCustomer_temp,(dfdimCustomer_silver.CustomerName == dfdimCustomer_temp.CustomerName) & (dfdimCustomer_silver.Email == dfdimCustomer_temp.Email), \"left_anti\")\n",
    "    \n",
    "dfdimCustomer_gold = dfdimCustomer_gold.withColumn(\"CustomerID\",monotonically_increasing_id() + MAXCustomerID + 1)\n",
    "\n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "\n",
    "display(dfdimCustomer_gold.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a45d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/dimcustomer_gold')\n",
    "    \n",
    "dfUpdates = dfdimCustomer_gold\n",
    "    \n",
    "deltaTable.alias('gold') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'gold.CustomerName = updates.CustomerName AND gold.Email = updates.Email'\n",
    "  ) \\\n",
    "   .whenMatchedUpdate(set =\n",
    "    {\n",
    "          \n",
    "    }\n",
    "  ) \\\n",
    " .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"CustomerName\": \"updates.CustomerName\",\n",
    "      \"Email\": \"updates.Email\",\n",
    "      \"First\": \"updates.First\",\n",
    "      \"Last\": \"updates.Last\",\n",
    "      \"CustomerID\": \"updates.CustomerID\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11590cdc",
   "metadata": {},
   "source": [
    "### create product diemnsion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "    \n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"sales.dimproduct_gold\") \\\n",
    "    .addColumn(\"ItemName\", StringType()) \\\n",
    "    .addColumn(\"ItemID\", LongType()) \\\n",
    "    .addColumn(\"ItemInfo\", StringType()) \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4ed74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, lit, when\n",
    "    \n",
    "# Create product_silver dataframe\n",
    "    \n",
    "dfdimProduct_silver = df.dropDuplicates([\"Item\"]).select(col(\"Item\")) \\\n",
    "    .withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n",
    "    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \n",
    "    \n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "\n",
    "display(dfdimProduct_silver.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28feddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, col, lit, max, coalesce\n",
    "    \n",
    "#dfdimProduct_temp = dfdimProduct_silver\n",
    "dfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n",
    "    \n",
    "MAXProductID = dfdimProduct_temp.select(coalesce(max(col(\"ItemID\")),lit(0)).alias(\"MAXItemID\")).first()[0]\n",
    "    \n",
    "dfdimProduct_gold = dfdimProduct_silver.join(dfdimProduct_temp,(dfdimProduct_silver.ItemName == dfdimProduct_temp.ItemName) & (dfdimProduct_silver.ItemInfo == dfdimProduct_temp.ItemInfo), \"left_anti\")\n",
    "    \n",
    "dfdimProduct_gold = dfdimProduct_gold.withColumn(\"ItemID\",monotonically_increasing_id() + MAXProductID + 1)\n",
    "    \n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "\n",
    "display(dfdimProduct_gold.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641c623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "    \n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/dimproduct_gold')\n",
    "            \n",
    "dfUpdates = dfdimProduct_gold\n",
    "            \n",
    "deltaTable.alias('gold') \\\n",
    "  .merge(\n",
    "        dfUpdates.alias('updates'),\n",
    "        'gold.ItemName = updates.ItemName AND gold.ItemInfo = updates.ItemInfo'\n",
    "        ) \\\n",
    "        .whenMatchedUpdate(set =\n",
    "        {\n",
    "               \n",
    "        }\n",
    "        ) \\\n",
    "        .whenNotMatchedInsert(values =\n",
    "         {\n",
    "          \"ItemName\": \"updates.ItemName\",\n",
    "          \"ItemInfo\": \"updates.ItemInfo\",\n",
    "          \"ItemID\": \"updates.ItemID\"\n",
    "          }\n",
    "          ) \\\n",
    "          .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056d52b",
   "metadata": {},
   "source": [
    "### create fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75affe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "    \n",
    "DeltaTable.createIfNotExists(spark) \\\n",
    "    .tableName(\"sales.factsales_gold\") \\\n",
    "    .addColumn(\"CustomerID\", LongType()) \\\n",
    "    .addColumn(\"ItemID\", LongType()) \\\n",
    "    .addColumn(\"OrderDate\", DateType()) \\\n",
    "    .addColumn(\"Quantity\", IntegerType()) \\\n",
    "    .addColumn(\"UnitPrice\", FloatType()) \\\n",
    "    .addColumn(\"Tax\", FloatType()) \\\n",
    "    .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e2751",
   "metadata": {},
   "source": [
    "### combine table for new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe34713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "    \n",
    "dfdimCustomer_temp = spark.read.table(\"Sales.dimCustomer_gold\")\n",
    "dfdimProduct_temp = spark.read.table(\"Sales.dimProduct_gold\")\n",
    "    \n",
    "df = df.withColumn(\"ItemName\",split(col(\"Item\"), \", \").getItem(0)) \\\n",
    "    .withColumn(\"ItemInfo\",when((split(col(\"Item\"), \", \").getItem(1).isNull() | (split(col(\"Item\"), \", \").getItem(1)==\"\")),lit(\"\")).otherwise(split(col(\"Item\"), \", \").getItem(1))) \\\n",
    "    \n",
    "    \n",
    "# Create Sales_gold dataframe\n",
    "    \n",
    "dffactSales_gold = df.alias(\"df1\").join(dfdimCustomer_temp.alias(\"df2\"),(df.CustomerName == dfdimCustomer_temp.CustomerName) & (df.Email == dfdimCustomer_temp.Email), \"left\") \\\n",
    "        .join(dfdimProduct_temp.alias(\"df3\"),(df.ItemName == dfdimProduct_temp.ItemName) & (df.ItemInfo == dfdimProduct_temp.ItemInfo), \"left\") \\\n",
    "    .select(col(\"df2.CustomerID\") \\\n",
    "        , col(\"df3.ItemID\") \\\n",
    "        , col(\"df1.OrderDate\") \\\n",
    "        , col(\"df1.Quantity\") \\\n",
    "        , col(\"df1.UnitPrice\") \\\n",
    "        , col(\"df1.Tax\") \\\n",
    "    ).orderBy(col(\"df1.OrderDate\"), col(\"df2.CustomerID\"), col(\"df3.ItemID\"))\n",
    "    \n",
    "# Display the first 10 rows of the dataframe to preview your data\n",
    "    \n",
    "display(dffactSales_gold.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44616fd",
   "metadata": {},
   "source": [
    "### make sure data stay updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0229e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "    \n",
    "deltaTable = DeltaTable.forPath(spark, 'Tables/factsales_gold')\n",
    "    \n",
    "dfUpdates = dffactSales_gold\n",
    "    \n",
    "deltaTable.alias('gold') \\\n",
    "  .merge(\n",
    "    dfUpdates.alias('updates'),\n",
    "    'gold.OrderDate = updates.OrderDate AND gold.CustomerID = updates.CustomerID AND gold.ItemID = updates.ItemID'\n",
    "  ) \\\n",
    "   .whenMatchedUpdate(set =\n",
    "    {\n",
    "          \n",
    "    }\n",
    "  ) \\\n",
    " .whenNotMatchedInsert(values =\n",
    "    {\n",
    "      \"CustomerID\": \"updates.CustomerID\",\n",
    "      \"ItemID\": \"updates.ItemID\",\n",
    "      \"OrderDate\": \"updates.OrderDate\",\n",
    "      \"Quantity\": \"updates.Quantity\",\n",
    "      \"UnitPrice\": \"updates.UnitPrice\",\n",
    "      \"Tax\": \"updates.Tax\"\n",
    "    }\n",
    "  ) \\\n",
    "  .execute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af53d0",
   "metadata": {},
   "source": [
    "## create semantic model\n",
    "\n",
    "In your workspace, navigate to your Sales lakehouse.\n",
    "Select New semantic model from the ribbon of the Explorer view.\n",
    "Assign the name Sales_Gold to your new semantic model.\n",
    "Select your transformed gold tables to include in your semantic model and select Confirm.\n",
    "\n",
    "    dimdate_gold\n",
    "    dimcustomer_gold\n",
    "    dimproduct_gold\n",
    "    factsales_gold\n",
    "\n",
    "#### ==> This will open the semantic model in Fabric where you can create relationships and measures, as shown here:"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54724ae8",
   "metadata": {},
   "source": [
    "# Analyze data with Apache Spark in Fabric\n",
    "https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/02-analyze-spark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64266685",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "1. define shcema for delta table\n",
    "2. batch load csv files from folder\n",
    "\n",
    "https://sparkbyexamples.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ef593",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import spark\n",
    "\n",
    "orderSchema = StructType([\n",
    "    StructField(\"SalesOrderNumber\", StringType()),\n",
    "    StructField(\"SalesOrderLineNumber\", IntegerType()),\n",
    "    StructField(\"OrderDate\", DateType()),\n",
    "    StructField(\"CustomerName\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"Item\", StringType()),\n",
    "    StructField(\"Quantity\", IntegerType()),\n",
    "    StructField(\"UnitPrice\", FloatType()),\n",
    "    StructField(\"Tax\", FloatType())\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(orderSchema).load(\"Files/orders/*.csv\")\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106cc79b",
   "metadata": {},
   "source": [
    "# Select columns and aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfbffeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opt 1\n",
    "customers = df['CustomerName', 'Email']\n",
    "# Opt 2\n",
    "customers = df.select(\"CustomerName\", \"Email\")\n",
    "\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "\n",
    "display(customers.distinct())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b021100",
   "metadata": {},
   "source": [
    "## filter with \"where\" clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = df.select(\"CustomerName\", \"Email\").where(df['Item']=='Road-250 Red, 52')\n",
    "print(customers.count())\n",
    "print(customers.distinct().count())\n",
    "\n",
    "display(customers.distinct())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cd1538",
   "metadata": {},
   "source": [
    "## aggregate with \"groupby\" clause and set name with \"alias\" clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda1bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "yearlySales = df.select(year(col(\"OrderDate\")).alias(\"Year\")).groupBy(\"Year\").count().orderBy(\"Year\")\n",
    "\n",
    "display(yearlySales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f1b70b",
   "metadata": {},
   "source": [
    "## transform columns with \"year\" and \"month\" function and \"getItem()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad1fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create Year and Month columns\n",
    "transformed_df = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n",
    "\n",
    "# Create the new FirstName and LastName fields\n",
    "transformed_df = transformed_df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n",
    "\n",
    "# Filter and reorder columns\n",
    "transformed_df = transformed_df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"Email\", \"Item\", \"Quantity\", \"UnitPrice\", \"Tax\"]\n",
    "\n",
    "# Display the first five orders\n",
    "display(transformed_df.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3e054",
   "metadata": {},
   "source": [
    "## Save and load \"parquet\" files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64530821",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df.write.mode(\"overwrite\").parquet('Files/transformed_data/orders')\n",
    "\n",
    "print (\"Transformed data saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read.format(\"parquet\").load(\"Files/transformed_data/orders\")\n",
    "display(orders_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a87fe94",
   "metadata": {},
   "source": [
    "### saved partitioned for better retreival --> many parquet file created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dcb06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write.partitionBy(\"Year\",\"Month\").mode(\"overwrite\").parquet(\"Files/partitioned_data\")\n",
    "\n",
    "print (\"Transformed data saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ef59fe",
   "metadata": {},
   "source": [
    "### selective load by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd5819",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_2021_df = spark.read.format(\"parquet\").load(\"Files/partitioned_data/Year=2021/Month=*\")\n",
    "\n",
    "display(orders_2021_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0981caea",
   "metadata": {},
   "source": [
    "# Create SQL table / load spark df from SQL table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c0384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new table\n",
    "df.write.format(\"delta\").saveAsTable(\"salesorders\")\n",
    "\n",
    "# Get the table description\n",
    "spark.sql(\"DESCRIBE EXTENDED salesorders\").show(truncate=False)\n",
    "\n",
    "df = spark.sql(\"SELECT * FROM lakehouse_name.salesorders LIMIT 1000\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44b8d33",
   "metadata": {},
   "source": [
    "## run SQL in notebook with magic command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10cfd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT YEAR(OrderDate) AS OrderYear,\n",
    "       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue\n",
    "FROM salesorders\n",
    "GROUP BY YEAR(OrderDate)\n",
    "ORDER BY OrderYear;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3ea098",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM salesorders\n",
    "# --> display graph with \"+\" options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ee6abf",
   "metadata": {},
   "source": [
    "## use matplotlib to plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlQuery = \"SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \\\n",
    "                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue, \\\n",
    "                COUNT(DISTINCT SalesOrderNumber) AS YearlyCounts \\\n",
    "            FROM salesorders \\\n",
    "            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \\\n",
    "            ORDER BY OrderYear\"\n",
    "df_spark = spark.sql(sqlQuery)\n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Clear the plot area\n",
    "plt.clf()\n",
    "\n",
    "# Create a figure for 2 subplots (1 row, 2 columns)\n",
    "fig, ax = plt.subplots(1, 2, figsize = (10,4))\n",
    "\n",
    "# Create a bar plot of revenue by year on the first axis\n",
    "ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')\n",
    "ax[0].set_title('Revenue by Year')\n",
    "\n",
    "# Create a pie chart of yearly order counts on the second axis\n",
    "ax[1].pie(df_sales['YearlyCounts'])\n",
    "ax[1].set_title('Orders per Year')\n",
    "ax[1].legend(df_sales['OrderYear'])\n",
    "\n",
    "# Add a title to the Figure\n",
    "fig.suptitle('Sales Data')\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ea9829",
   "metadata": {},
   "source": [
    "## use seaborn to plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Clear the plot area\n",
    "plt.clf()\n",
    "\n",
    "# Set the visual theme for seaborn\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create a bar chart\n",
    "ax = sns.barplot(x=\"OrderYear\", y=\"GrossRevenue\", data=df_sales)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d33d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
